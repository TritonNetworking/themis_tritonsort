# ====
# Benchmark experiment parameters
# ====
BENCHMARK_DATA_SIZE_PER_NODE:
  mixediobench: 10000000000 # 10 GB
PARTITION_SIZE: 1000000000

INPUT_DISK_LIST:
  mixediobench: "/mnt/disks/fioc/USERNAME/storagebench"
OUTPUT_DISK_LIST:
  mixediobench: "/mnt/disks/fiod/USERNAME/mixediobench"

# Comma delimited list of IPs
PEER_LIST: 192.168.1.5
NUM_INTERFACES: 1

# ====
# Core assignments and thread counts
# ====

THREAD_CPU_POLICY:
  mixediobench:
    DEFAULT:
      type: "free"
      #mask:"012345678901234567890123456789012345678901234567"
      mask: "111111111111111111111111111111111111111111111111"
   #reader:
   #tagger:
   #sender:
   #receiver:
   #demux:
   #writer:

NUM_WORKERS:
  mixediobench:
    reader: 1
    tagger: 1
    sender: 1
    receiver: 1
    demux: 1
    writer: 1

# ====
# Memory parameters
# ====
DEFAULT_BUFFER_SIZE:
  mixediobench:
    reader: 4194304

MEMORY_QUOTAS:
  mixediobench:
    reader: 1000000000 #1GB
    receiver: 1000000000 #1GB

# ====
# Storage parameters
# ====

DIRECT_IO:
  mixediobench:
    reader: 1
    writer: 1

ALIGNMENT_MULTIPLE: 512

FILE_PREALLOCATION: 1

DELETE_AFTER_READ:
  mixediobench: 0

# If using asynchronous reads or writes, these values determine how many
# operations of the appropriate type can be in flight simultaneously from
# each worker thread.
#
# It should be noted that the implementation of the asynchronous reader causes
# each file to be read one buffer at a time, which means a depth of D will cause
# D files to be read simultaneously, not D reads from a single file. By
# contrast, the asynchronous writer operates at the buffer level, so a depth of
# D could cause D buffers for the same file to be written simultaneously.
ASYNCHRONOUS_IO_DEPTH:
  mixediobench:
    reader: 2
    writer: 4

# ====
# Networking parameters
# ====
SOCKETS_PER_PEER:
  mixediobench:
    sender: 1

SEND_SOCKET_SYSCALL_SIZE: 16777216
RECV_SOCKET_SYSCALL_SIZE: 16777216

# Setting these to 0 means don't modify the TCP buffer sizes
TCP_SEND_BUFFER_SIZE: 0
TCP_RECEIVE_BUFFER_SIZE: 0

# ====
# Worker implementations
# ====

# Reader and writer can be changed to use asynchronous or synchronous I/O.
#
# reader:
# ByteStreamReader - synchronous reads
# LibAIOReader - async reads using libaio
# PosixAIOReader - async reads using posix AIO
#
# writer:
# Writer - synchronous writes
# LibAIOWriter - async writes using libaio
# PosixAIOWriter - async writes using posix AIO
#
# receiver can use polling or select
# Receiver - polling
# SelectReceier - select
WORKER_IMPLS:
  mixediobench:
    reader: "ByteStreamReader"
    tagger: "Tagger" # Do not modify
    sender: "Sender" # Do not modify
    receiver: "Receiver"
    demux: "Demux" # Do not modify
    writer: "Writer"

# ====
# General config
# ====

MONITOR_PORT: 9999
RECEIVER_PORT: 9000

ENABLE_STAT_WRITER: true
STAT_WRITER_DRAIN_INTERVAL_MICROS: 500000
STAT_POLL_INTERVAL: 500000

MEM_SIZE: 128000000000
CORES_PER_NODE: 48

# ====
# Benchmark pipeline specific. Do not modify
# ====

WORK_QUEUEING_POLICY:
  mixediobench:
    reader: "ReadRequestWorkQueueingPolicy"
    tagger: "ByteStreamWorkQueueingPolicy"
    sender: "NetworkDestinationWorkQueueingPolicy"
    writer: "PhysicalDiskWorkQueueingPolicy"

# Readers use the existance of this flag to read into byte stream buffers
FORMAT_READER.mixediobench: "NULL"

ALIGNMENT:
  mixediobench:
    reader: 0
    receiver: 0
    writer: 0

# ====
# Copied from mapreduce/defaults.yaml
# ====

# Print header for status log messages
CHANNEL_STATUS_HEADER: "[STATUS]"

# Print header for statistics log messages
CHANNEL_STATISTIC_HEADER: "[STATISTIC]"

# Print header for parameter log messages
CHANNEL_PARAM_HEADER: "[PARAM]"
